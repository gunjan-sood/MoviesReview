{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movies Review Data\n",
    "\n",
    "The problem contains the dataset which includes the movies review data with review as one column and the sentiment(positive or negative) associated with it in another column.\n",
    "\n",
    "The objective is to perform sentiment analysis on the reviews and build a model to do sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import all the required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the positive and negative reviews dataset and merge them to create a new dataset with all the reviews. Mark all the negative reviews as 0 and positive reviews as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>simplistic , silly and tedious .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>it's so laddish and juvenile , only teenage bo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>exploitative and largely devoid of the depth o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[garbus] discards the potential for pathologic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>a visually flashy but narratively opaque and e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                             review  sentiment_label\n",
       "0      0                  simplistic , silly and tedious .                 0\n",
       "1      1  it's so laddish and juvenile , only teenage bo...                0\n",
       "2      2  exploitative and largely devoid of the depth o...                0\n",
       "3      3  [garbus] discards the potential for pathologic...                0\n",
       "4      4  a visually flashy but narratively opaque and e...                0"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the positive and negative reviews dataset and create a dataset with all the reviews\n",
    "\n",
    "neg = pd.read_csv('rt-polarity-neg.csv', sep='\\n', header=None, names=['review'])\n",
    "pos = pd.read_csv('rt-polarity-pos.csv', sep='\\n', header=None, names=['review'])\n",
    "neg['sentiment_label']=0\n",
    "pos['sentiment_label']=1\n",
    "reviews_df=neg.append(pos)\n",
    "reviews_df.reset_index(inplace=True)\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis\n",
    "\n",
    "To understand the data, carry out some exploratory analysis: by checking the datatypes of the variables, size of the dataset and if there are any null values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10662 entries, 0 to 10661\n",
      "Data columns (total 3 columns):\n",
      "index              10662 non-null int64\n",
      "review             10662 non-null object\n",
      "sentiment_label    10662 non-null int64\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 250.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# check the dataset\n",
    "reviews_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index              0\n",
       "review             0\n",
       "sentiment_label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if there are any missing values in the dataset\n",
    "reviews_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the index column from the dataset\n",
    "reviews_df.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simplistic , silly and tedious .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it's so laddish and juvenile , only teenage bo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exploitative and largely devoid of the depth o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[garbus] discards the potential for pathologic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a visually flashy but narratively opaque and e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment_label\n",
       "0                  simplistic , silly and tedious .                 0\n",
       "1  it's so laddish and juvenile , only teenage bo...                0\n",
       "2  exploitative and largely devoid of the depth o...                0\n",
       "3  [garbus] discards the potential for pathologic...                0\n",
       "4  a visually flashy but narratively opaque and e...                0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "Next, split the available dataset into training and test data with 10% of the total data assigned as the test dataset and remianing 90% as the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = reviews_df['review']\n",
    "y = reviews_df['sentiment_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create train and test dataset\n",
    "train_review_df = reviews_df.ix[X_train.index]\n",
    "test_review_df = reviews_df.ix[X_test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape (9595, 2)\n",
      "Test dataset shape (1067, 2)\n"
     ]
    }
   ],
   "source": [
    "print 'Training dataset shape',train_review_df.shape\n",
    "print 'Test dataset shape',test_review_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Import Natural language toolkit to clean the reviews by removing punctuations or numbers etc. Though punctuations may help to express the sentiments in some cases but not taking them into consideration for now. \n",
    "Import the stopwords list to remove all the stopwords from the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# import the libraries \n",
    "\n",
    "import nltk # Import the stop word library from python Natural Language Toolkit\n",
    "nltk.download()\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "import re # Import regular expression library to find and replace the words\n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to convert uncleaned reviews to a string of reviews \n",
    "# 1. Remove non letters\n",
    "# 2. Change everything to lowercase\n",
    "# 3. Remove stopwords\n",
    "\n",
    "def cleanup_reviews(review):\n",
    "    letters = re.sub(\"[^a-zA-Z]\", \" \", review) # Remove anything in the sentence other than letters\n",
    "    words = letters.lower().split()   # change everything to lowercase\n",
    "    stops = set(stopwords.words(\"english\")) # convert to a set for faster processing\n",
    "    meaningful_words = [w.strip() for w in words if not w in stops]   # remove the stop words\n",
    "    sentence = \" \".join( meaningful_words )  # join back all the remaining words into sentence separated by a space\n",
    "    return sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply the cleanup_review function to all the reviews in the training dataset\n",
    "train_review_df['clean_review'] = train_review_df['review'].apply(cleanup_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_review_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the cleaned training dataset as a pickle\n",
    "train_review_df.to_pickle(\"cleaned_movie_reviews2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply the cleanup_review function to all the reviews in the test dataset\n",
    "test_review_df['clean_review'] = test_review_df['review'].apply(cleanup_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_review_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the cleaned test dataset as a pickle\n",
    "train_review_df.to_pickle(\"cleaned_movie_reviews2_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline -tfidf and classifier\n",
    "\n",
    "Generate feature matrix using tf-idf vectorization based on term frequency and inverse document frequency instead of using the bag of words which simply counts the word frequency in a sentence.\n",
    "\n",
    "Using Pipeline functionality to merge the feature extraction and classification steps into one operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),\n",
    "#       ('clf', RandomForestClassifier(n_estimators=200)),\n",
    "#       ('clf', MultinomialNB()), \n",
    "       ('clf', LogisticRegression()),\n",
    "#       ('clf',  KNeighborsClassifier())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation - Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_fold = KFold(n=len(train_review_df['clean_review']), n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[3588 1201]\n",
      " [1211 3595]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.76      0.75       542\n",
      "          1       0.75      0.74      0.74       525\n",
      "\n",
      "avg / total       0.75      0.75      0.75      1067\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion = np.array([[0, 0], [0, 0]])\n",
    "scores = []\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = train_review_df.iloc[train_indices]['clean_review']\n",
    "    train_y = train_review_df.iloc[train_indices]['sentiment_label']\n",
    "\n",
    "    valid_text = train_review_df.iloc[test_indices]['clean_review']\n",
    "    valid_y = train_review_df.iloc[test_indices]['sentiment_label']\n",
    "\n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(valid_text)\n",
    "\n",
    "    confusion += confusion_matrix(valid_y, predictions)\n",
    "\n",
    "print('Confusion matrix:')\n",
    "print(confusion)\n",
    "\n",
    "predicted_results = pipeline.predict(test_review_df['clean_review'])\n",
    "print '\\n'\n",
    "print(classification_report(test_review_df['sentiment_label'], predicted_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "<pre>\n",
    "---\n",
    "|  | Logistic Regression    |  Naive Bayes    |  Random Forest    | KNN     |\n",
    "|------|------|------|------|------|-------|------|------|\n",
    "|  Bag of words -5000 features, uni-bigram  |0.52| 0.50 | 0.52 |  0.42  | \n",
    "|  Bag of words -5000 features  |0.50| 0.50 | 0.51 |  0.36  | \n",
    "|  tf-idf   |0.49| 0.50 | 0.49 | 0.48   | \n",
    "|  tf-idf + kfold=5  |0.75| 0.75 | 0.69 |  0.38  |\n",
    "\n",
    " Logistic Regression > Naive Bayes > Random Forest > KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO:\n",
    "\n",
    "* Not removing the stop words and then trying ngram\n",
    "* Representing word cloud - Most and least used words in positive and negative reviews\n",
    "* Try other ML models like SVM, Naive Baye's other forms, Boosting\n",
    "* Data Visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
